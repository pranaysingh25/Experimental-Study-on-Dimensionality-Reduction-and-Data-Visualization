{"cells":[{"metadata":{},"cell_type":"markdown","source":"**OBJECTIVES:**\n\n**1. Dimensionality Reduction for performance:** *Does PCA really helps ? ..in what ways? Does it improves training time, performance accuracy?*\n\n**2. Dimensionality Reduction for Visualization:** *What are different Dimensionality Reduction methods for visualization and how better or worse are they in terms of speed and visualization.*\n    \n> *We will try to answer these questions by working around the MNIST dataset*"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#importing python essential libraries, and our MNIST dataset.\n\nimport numpy as np\nimport pandas as pd\ntest = pd.read_csv(\"../input/mnist-data/mnist_test.csv\")\ntrain = pd.read_csv(\"../input/mnist-data/mnist_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#separating the labels\n\ny_train = train['label']\nX_train = train.drop('label', axis = 1)\n\ny_test = test['label']\nX_test = test.drop('label', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So, we have kept 60000 images for training and 10000 for testing\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**PCA compression**\n\n*Let's compress our data using PCA Reduction and then decompress it again to see the 'loss' by plotting the actual image*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's first make the copy of our original training data before any transformations\nX_train_ = X_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting a sample image\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nplt.imshow(X_train.iloc[10000].to_numpy().reshape(28,28))\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#just to be assured!\ny_train[10000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will keep track of the time taken to perform each transformation and training.\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's compress our data using PCA to a degree that preserves 95% variance of the data and only looses 5%."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 0.95)\nstart = time.time()\nX_reduced = pca.fit_transform(X_train)\nend = time.time()\n\nend - start","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">PCA took 7 seconds to compress the data! Noted."},{"metadata":{"trusted":true},"cell_type":"code","source":"#so, only 154 out of 784 features can preserve 95% of the data, \n#this means the MNIST is originally very sparse and most of the data is rather present at a much lower dimension.  \npca.n_components_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">let's now reverse back the compression, using PCA reverse transform, While it reverses the dataset back to having 784 features\nbut the information lost(5%) due to compression never gets recovered. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_decompress = pca.inverse_transform(X_reduced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now that we have got both compressed and decompressed data, let's plot them side by side to see how much we lost(5%)\nfig, (ax1, ax2) = plt.subplots(1,2)\nax1.imshow(X_train.iloc[10000].to_numpy().reshape(28,28))\nax2.imshow(X_decompress[10000].reshape(28, 28))\nfig.suptitle('compression and decompression')\nax1.axis('off')\nax2.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it seems that 5% loss(since we set preservance to 95%) was not a bad deal, the image still looks acceptable and we have significantly reduced the number of dimensions as well, from 784 to 154 trading for a 5% loss in image quality!"},{"metadata":{},"cell_type":"markdown","source":"    1. Dimensionality Reduction for performance: Does PCA helps?"},{"metadata":{},"cell_type":"markdown","source":"We have seen PCA helps in reducing the size of data upto a harmless level. Now, does this really helps in the training ? \nDoes reduced number of dimensions helps in faster training ? Does reduced dataset generalize better or worse?\n\nLets try on a couple of classifiers to find the answers\n\n**The Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier: Random Forest on Original data\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100, random_state = 42)\nt0 = time.time()\nrfc.fit(X_train_, y_train)\nt1 = time.time()\n\nt1-t0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#score on random forest\nrfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest took 59 sec to train the original data with 784 features and that gave a score of 97%!**\n\n> Now let's check how does PCA the time and accuracy with Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"#classifier: Random Forest on Reduced data\n\nrfc2 = RandomForestClassifier(n_estimators = 100, random_state = 42)\nt0 = time.time()\nrfc2.fit(X_reduced, y_train)\nt1 = time.time()\n\nt1-t0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#score of 'random forest' on Reduced data\n\nX_test_reduced = pca.transform(X_test)\nrfc2.score(X_test_reduced, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**That's bad! Training took more than twice the time it took on original dataset. There is a drop in performance as well!\nSo PCA didn't really help in this case. But why did this happen?\nLet's try this on one more classifier before concluding the observations.** \n\n**The Softmax Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classifier: Logistic Regression on Original data\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 42)\nt0 = time.time()\nlog_clf.fit(X_train_, y_train)\nt1 = time.time()\n\nt1-t0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#score of 'Logistic Regression' on Original data\nlog_clf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wait what! Softmax actually took half of the time taken by the Random Forest on the original dataset with a slight drop in performance. \nBut our aim is to see what's tha case of PCA with the Softmax Classifier.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#time to train on 'Logistic Regression' : Reduced data\n\nlog_clf2 = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs', random_state = 42)\nt0 = time.time()\nlog_clf2.fit(X_reduced, y_train)\nt1 = time.time()\n\nt1-t0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*wow! Softmax Classifier is 2 times faster on reduced data than on original data. Let's check the model's accuracy as well *"},{"metadata":{"trusted":true},"cell_type":"code","source":"#score of 'Logistic Regression' on Reduced data\nlog_clf2.score(X_test_reduced, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**That's even great! Softmax Classifier trains faster on the reduced data with a extremely minimal drop in performance for 2x speed!**"},{"metadata":{},"cell_type":"markdown","source":"> **Findings:** \n* We saw that PCA didn't help the Random Forest classifier, it instead made the training slow and worsened the performance of the classifier.\n* While in case of Softmax Classifier, PCA helped it gain 2 times faster speed with almost similar performance.\n\n> **Conclusions:**\n* There you have it clearly, PCA helps..but not always\n* Infact, Dimensionality Reduction does not always leads to faster training, it rather depends on the dataset, the model and the training algorithm used."},{"metadata":{},"cell_type":"markdown","source":"    2. Dimensionality Reduction for Visualization: Studying and comparing Visualizations using different methods of Reduction"},{"metadata":{},"cell_type":"markdown","source":"**Using TSNE**\n\nSince TSNE scales extremely slowly with large dataset, we will not use the full data, rather a sample of just 10000 instances for study purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating random 10000 samples of the training data\nX_train['label'] = y_train\nX = X_train.sample(n=10000, random_state=42)\n\ny = X['label']\nX = X.drop('label', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use TSNE to reduce the datset down to 2 Dimensions and then plot it using Matplotlib\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components = 2, random_state = 42)\nt0 = time.time()\nX_reduced = tsne.fit_transform(X)\nt1 = time.time()\n\nt1-t0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wuuf! it took almost 5 minutes for TSNE to compress the data(only with 10000 samples) to 2 dimensions. Now lets plot it."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.scatter(X_reduced[:,0], X_reduced[:,1], c = y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks quite nice! we can see clear separation of clusturs. \nwhile only a couple of these clusters seems to overlap, like 3s & 5s and 9s & 4s.\nWe can try to run PCA on select of them like 9s and 4s"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = (y == 4) | (y == 9) \nX_subset = X[idx]\ny_subset = y[idx]\n\ntsne_subset = TSNE(n_components=2, random_state=42)\nX_subset_reduced = tsne_subset.fit_transform(X_subset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\nfor digit in (4, 9):\n    plt.scatter(X_subset_reduced[y_subset == digit, 0], X_subset_reduced[y_subset == digit, 1] )\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While they do not overlap as much, still the clusters are stuck together tightly!\n\nLet's try some other visualization techiniques, let's go with PCA for visualization.\n\n**PCA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nt0 = time.time()\nX_pca = pca.fit_transform(X)\nt1 = time.time()\nprint(t1-t0)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_pca[:,0], X_pca[:,1], c = y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA is extremely fast! But we can clearly see how poor its visualization is. Almost everything is overlapped. \n\n\nWhat would happen if we combine PCA and TSNE? would that give any advantage? let's check it out\n\n**PCA + TSNE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npca_tsne = Pipeline([\n    ('pca', PCA(n_components=0.95, random_state=42)),\n    ('tsne', TSNE(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_tsne = pca_tsne.fit_transform(X)\nt1 = time.time()\nprint(t1-t0)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_reduced[:,0], X_reduced[:,1], c = y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*WOW! Did you notice what happened ? The result is quite exactly similar to that of using TSNE alone. But the time is reduced to about half of orginial time(just TSNE).*\n\n*But how did that happen ?*\n\n**Well, we already saw earlier that PCA is very fast compressor than others when it comes to large datasets, but algorithms like TSNE creats far better clusters than PCA, it make sense combining PCA(to quickly get rid of useless dimensions) and TSNE(a slower reduction algorithm reducing less heavy data to 2 Dimensions to make good clusters). This can significantly reduce the time.**"},{"metadata":{},"cell_type":"markdown","source":"Let's now try on few other methods of dimensionality reduction\n\n**LLE: Locally Linear Embedding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import LocallyLinearEmbedding\nt0 = time.time()\nX_lle = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(t1 - t0)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_lle[:,0], X_lle[:,1], c = y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It took a while, and also, the visualization is not at all appealing. \n\nlet's now chain this with PCA\n\n**PCA + LLE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_lle = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"lle\", LocallyLinearEmbedding(n_components=2, random_state=42)),\n])\nt0 = time.time()\nX_pca_lle = pca_lle.fit_transform(X)\nt1 = time.time()\nprint(t1-t0)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_pca_lle[:,0], X_pca_lle[:,1], c = y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, while the results were same, the time was quite reduced. That's what we had expected!"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's try a last one! LDA\n\n**LDA: Linear Discriminant Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nt0 = time.time()\nX_lda = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\nt1 = time.time()\nprint(t1 - t0)\n\nplt.figure(figsize=(12, 8))\nplt.scatter(X_pca_lle[:,0], X_pca_lle[:,1], c = y, cmap='jet')\nplt.colorbar()\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! this was quite faster! Although the clusturs are still not that great!\n\n**That's all! We have tried exploring various Dimensionality Reduction Algorithms and seen how they perform. \nAnd I guess, We have a clear Winner here, yes..that's TSNE !! It was faster when chained with PCA and the results as well were quite better than others.**"},{"metadata":{},"cell_type":"markdown","source":"**Findings:** \n1. TSNE out-performed other algorithm at making clear clusters.\n2. PCA helped other algorithms to perform faster reduction.\n3. PCA scales faster than other algorithms but is not that good in creating clusters.\n4. Manifold based algorithms scale very poorly with larger dataset, hence are very slow.\n\n**Conclusions:**\n1. Manifold based reduction methods scale very poorly with larger dataset, hence are very slow.\n2. Chaining PCA with Manifold based reduction methods can help them scale faster."},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------\n*That's all folks! :)*\n\n*Thanks*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}